%%----------Chapter 2------------------------------------------
\chapter{How  It Works}\label{chap:background}
Measuring the movement of bodies in three dimensional space is not a new field of study.
Especially in our current age of electronics, one can simply grab an accelerometer and microcontroller off of the Internet, have it delivered in less than 96 hours, and know the accelerations felt on a roller coaster or be able to orient a 3D rendering of a bunny \footnote[1]{https://learn.adafruit.com/adafruit-bno055-absolute-orientation-sensor/overview}.
What makes today exciting is that improvements in manufacturing and computational density have drastically increased the capabilities of small embedded systems.
Now, every cell phone has a sophisticated suite of sensors built into them that have the simple task of detecting whether the phone is in a landscape or portrait mode. 
Some applications, like Google Maps, have gone further and fused the sensor data, along with GPS information and computer vision, into a robust pedestrian navigation system \footnote[2]{https://gizmodo.com/i-tried-google-maps-experimental-walking-directions-of-1833225629}.

\section{Orientation and Rotation} \label{sec:bkg_orientation}
A body can be rotated in 3D space along the x-, y-, and z-axis.
\textbf{Roll ($\phi$)} defines rotation about the x-axis; \textbf{pitch ($\theta$)} defines rotation about the y-axis; and \textbf{yaw ($\psi$)} defines rotation about the z-axis.
These three rotations are known as Euler angles.

A body's orientation is always relative to another coordinate frame.
Consider a person standing straight and still on the Earth's surface.
Each of their appendages could be considered at some orientation relative to their body.
At rest and in their local coordinate frame, the person's body may not be considered moving or rotating.
But, when the scope is expanded to encompass the entire body and the Earth's frame, that person is now rotating about the Earth coordinate frame at almost 17,000 MPH.

\begin{figure}[h!]
    \caption[Body rotations]{Basic look at a 3D body and the axes of rotation in the body reference frame.}
    \label{fig:body_rotations}
    \centering
    \includegraphics[height=2.5in]{background/body_rotations.png}
\end{figure}

\begin{figure}
    \begin{fitbox}[frametitle=Aside: Notation and Nomenclature for Rotations]
        Bodies on their own typically use the "X, Y, Z" notation that is ubiquitous for the Cartesian coordinate system.
        However, when refering to planetary bodies like the Earth, this nomenclature typically changes to North, East, and Down (NED).
        This occurs because on the surface of a large spherical body, we can assume the local area is a plane tangential to the surface.
        To keep the broader scope in mind, we arbitrarily associate the X-axis with East, the Y-axis with North, and the Z-axis with Down towards the Earth's core (perpendicular to the surface).

        [TODO: INSERT GRAPHIC ON BODY AND PLANET COORDINATE FRAME]
    \end{fitbox}
\end{figure}

\subsection{Rotation Matrices}
A rotation matrix is a mathematical model for translating one body's inertial reference frame to another, e.g. local body to the global body.
These are used for Eulerian transformations of vectors.
The matrix is an $N \times N$ orthogonal matrix where $N$ is the number of dimensions in the vector;
for a three dimensional vector, the rotation matrix to go from one coordinate frame to another would be a $3 \times 3$ matrix.

[TODO: INSERT FIGURE ON FRAME ROTATIONS]

In Figure \ref{fig:frame_rotation}, we can see a base coordinate frame, $A$, and a different coordinate frame, $B$ that is rotated relative to the base frame about the axis, $\vec{r}$.
To express the rotation from the base to the local frame, we can borrow notation from Craig [CITE - Introduction to Robotics].
The "from" or "base" frame is the preceding superscript while the "to" or "local" frame is the preceding subscript, as shown below:

\begin{equation*}
    {}^A_B R = \left[
        \begin{matrix}
            r_{11} & r_{12} & r_{13} \\
            r_{21} & r_{22} & r_{23} \\
            r_{31} & r_{32} & r_{33}
        \end{matrix}
    \right] = \left[
        \begin{matrix}
            {}^Ax_b & {}^Ay_B & {}^Az_B
        \end{matrix}\right]
\end{equation*}

When we want to rotate a vector between frames, we must choose an order in which to do so.
For example, if we wanted to rotate from frame $A$ to frame $B$ using a yaw-pitch-roll rotation order, we can construct the following rotation matrix from Appendix \ref{sec:3d_rot_mat}:

\begin{align*}
    {}^A_B R = R_z(\psi) R_y(\theta) R_x(\phi) &= \left[
        \begin{matrix}
            \cos\psi & -\sin\psi & 0 \\
            \sin\psi & \cos\psi & 0 \\
            0 & 0 & 1
        \end{matrix}\right]
        \left[\begin{matrix}
            \cos\theta & 0 & \sin\theta \\
            0 & 1 & 0 \\
            -\sin\theta & 0 & \cos\theta
        \end{matrix}\right]
        \left[\begin{matrix}
            1 & 0 & 0 \\
            0 & \cos\phi & -\sin\phi \\
            0 & \sin\phi & \cos\phi 
        \end{matrix}\right] \\ \\
        &= \left[\begin{matrix}
            \cos\psi\cos\theta & \cos\psi\sin\theta\sin\phi-\sin\psi\cos\theta & \cos\psi\sin\theta\cos\phi+\sin\psi\sin\phi \\
            \sin\psi\cos\theta & \sin\psi\sin\theta\sin\phi+\cos\psi\cos\phi & \sin\psi\sin\theta\cos\phi-\cos\psi\sin\phi \\
            -\sin\theta & \cos\theta\sin\phi & \cos\theta\cos\phi
        \end{matrix}\right]
\end{align*}

Then, we can rotate the vector from $A$ to $B$ via:

\begin{equation*}
    {}^A\vec{v}_B = {}^A_B R \cdot \vec{v}_A
\end{equation*}

\paragraph*{Gimbal Lock} \label{par:gimbal_lock}
When two axes become parallel, Eulerian changes to either axis become irrelevant, ergo, the system loses a degree of freedom.
This condition is called "gimbal lock".
When in gimbal lock, rotations yield discontinuities that need to be mitigated by changing the rotation order, or moving two or three axes at once.
This can create strange pathways for the body and cause unexpected behavior.
Monitoring for and breaking gimbal lock are also computationally expensive as the program must perform multiple matrix multiplications and then have logic to determine if a) it is in a gimbal lock condition and b) what rotation order would be required to break the condition.

The figure below shows a body's rotation while in gimbal lock. The rotation order is roll-pitch-yaw.
First, it is pitched 90 degrees upwards to align the roll and yaw axes, $\langle0, 90, 0\rangle$ (Figure \ref{subfig:ship_009000}).
If we then want it's orientation to be $\langle90, 0, 0\rangle$ (Figure \ref{subfig:ship_900000}), the body would be rolled left 90 degrees and pitched down 90 degrees.
However, due to gimbal lock, the body ended up in the orientation $\langle0, 0, 90\rangle$ (Figure \ref{subfig:ship_000090})!
To break the gimbal lock and get to the correct orientation, the rotation order would have to be changed to pitch-yaw-roll, or the roll and yaw axes driven simultaneously.

\begin{figure}[h!]
    \centering
    \subfloat[Start: $\langle0, 0, 0\rangle$]{\includegraphics[width=0.25\textwidth]{background/ship_000000}\label{subfig:ship_000000}}\hskip3ex
    \subfloat[$\langle0, 90, 0\rangle$]{\includegraphics[height=1in]{background/ship_009000}\label{subfig:ship_009000}}\hskip10ex
    \subfloat[$\langle90, 90, 0\rangle$]{\includegraphics[height=1in]{background/ship_909000}\label{subfig:ship_909000}}\hskip5ex
    \subfloat[End: $\langle0, 0, 90\rangle$]{\includegraphics[width=0.25\textwidth]{background/ship_000090}\label{subfig:ship_000090}}
    \subfloat[Desired: $\langle90, 0, 0\rangle$]{\includegraphics[width=0.25\textwidth]{background/ship_900000}\label{subfig:ship_900000}}
    \caption[Gimbal lock demonstration]{Demonstration of gimbal lock on a rotating 3D body.
    Despite only pitching and rolling the body, the end result of the transformation is an effective yaw.
    This is because the first pitch aligns the roll and yaw axes, effectively making them the same.
    In the given rotation order, any roll rotation would be equivalent to a yaw rotation when the object is pitched 90$^{\circ}$.
    \textit{3D model courtesy of "printable models" from Free3D.com.}}
\end{figure}

\subsection{Quaternions} \label{ssec:quaternions}
In the 19th century, an Irish mathematician was contemplating the problem of Euler angles and analyzing three dimensional geometry.
William Hamilton considered the problem from within the complex or imaginary space and found that a three dimensional vector could not be expressed with a complex triplet.
Instead, Hamilton discovered that the problem could be solved with a complex quadruplet, called a quaternion [CITE - On Quaternions], which could be used as a more mathematically pure tool to understand three dimensional geometry.
For more detailed notes, see Appendix \ref{chap:quaternions}.

A quaternion exists in four dimensional Hamiltonian space and has three complex and one real component in its vector. 
Like a rotation matrix, it describes the rotation from one reference frame to another about some axis, $\vec{r}$, as shown in Figure \ref{fig:frame_rotation}.
But, the quaternion is not susceptible to gimbal lock as when two of its axes align, it still retains three degrees of freedom.
Additionally, it is not limited to the range of 0 to 360 (or -180 to 180) like Euler angles and therefore has no discontinuities.
We can express the quaternion from frame $A$ to frame $B$ as:

\begin{equation*}
    {}^A_B \vec{q} = \left[
        \begin{matrix}
            q_1 & q_2 & q_3 & q_4
        \end{matrix}
    \right] = 
    \left[
        \begin{matrix}
            w & x & y & z
        \end{matrix}
    \right] = 
    w + x\hat{i} + y\hat{j} + z\hat{k}
\end{equation*}

Since the quaternion is just a vector, this drastically simplifies any rotation calculations and improves computational performance.
In order to rotate a vector using a quaternion, we need to set the quaternion to be a unit quaternion by:

\begin{equation*}
    q = \frac{q}{|q|}
\end{equation*}

Then, we can calculate the inverse quaternion, ${q}^{-1}$ (or conjugate $q^*=q^{-1}, \text{ if } |q|=1)$, using:

\begin{equation*}
    q^{-1} = \frac{q_0 - q_1\hat{i} - q_2\hat{j} - q_3\hat{k}}{|q|^2}
\end{equation*}

Then, we can rotate the vector using the equation:

\begin{equation*}
    {}^A \vec{v}_B = {}^A_Bq \vec{v}_A {}^A_Bq^{-1}
\end{equation*}

In an inertial measurement unit or attitude and heading reference system, we will need to rotate freely between readings taken in the body's local frame to that in the global frame, such as determining linear acceleration.
Using the quaternion method above we can also relate body measurements to the global frame, which can be useful for other analyses.

\section{Sensing} \label{sec:sensing}
Now that we have a basic understanding of some of the mathematical relationships present in inertial tracking, we need to start sensing our environment.
So, how can we compute the position, velocity, acceleration, and attitude of a body in space?
In order to answer this question, we must first examine the different sensors that are available.

\subsection{Magnetometer} \label{ssec:magnetometer}
Magnetometers use magnetoresistive elements that change their effective resistance in the presence of a magnetic field [CITE - Robotics, Vision and Control].
Atoms within a magnetoresistive element change their orientations with the magnetic field.
The new orientation can hinder or aid the path of free electrons moving through the element, thus changing the resistance.
By measuring this value and correlating it to a measurement scale, the local magnetic field can be determined.

\begin{figure}[h!]
    \caption[Magnetometer block diagram]{Basic block diagram of a single-axis magnetometer where electrons are flowing through the magnetoresistive material. The left diagram shows the condition when the magnetic field is aligned (minimal resistance); the right shows the non-aligned magnetic field condition which increases the resistance the electrons face passing through the material.}
    \label{fig:magnetometer}
    \centering
    \includegraphics[width=4.5in]{background/magnetometer.png}
\end{figure}

\subsection{Gyroscope} \label{ssec:gyroscope}
A gyroscope is an inertial sensor that measures the angular velocity of a rotating body.
MEMS-based gyroscopes measure this value by applying the Coriolis effect on a microscopic mass [CITE - Robotics, Vision and Control].
As shown in Figure \ref{fig:gyroscopes}, an oscillation is induced on the x-axis using a driving circuit.
While oscillating, if an angular velocity ($\omega$) is imparted on the z-axis, the suspended mass will experience a force in the y-axis that is proportional to $\omega$ (Appendix [INSERT APPENDIX NUMBER]).
Since the mass is suspended on springs, Newton's Second Law can be applied again to directly correlate the force experienced to a displacement and therefore a change in resistance or capacitance.

\begin{figure}[h!]
    \caption[Gyroscope block diagram]{Basic block diagram of a three-axis gyroscope where the masses are suspended from springs. The left diagram shows a single mass configuration; the right shows a tuning fork configuration which is twice as sensitive as the singular mass.}
    \label{fig:gyroscopes}
    \centering
    \includegraphics[width=4.5in]{background/gyroscopes.png}
\end{figure}

\subsection{Accelerometer} \label{ssec:accelerometer}
Accelerometers measure a change in velocity over time (acceleration).
An accelerometer is comprised of a mass suspended in an axis of motion by springs of a known K-constant [CITE - Robotics, Vision and Control].
From Newton's Second Law, as applied to a spring-mass system, $\frac{-K}{m}x=a$, we can directly correlate the displacement of the mass to the magnitude of the acceleration along the measurement axis.
Typically, this scale is electrically resistive or capacitative and creates an analog change in a driving (exciting) voltage which can be measured in a circuit using a Wheatstone Bridge and microcontroller.
A basic representation of a three-axis accelerometer is shown in Figure \ref{fig:accelerometers}.

\begin{figure}[h!]
    \caption[Accelerometer block diagram]{Basic block diagram of a three-axis accelerometer where the masses are suspended from springs.}
    \label{fig:accelerometers}
    \centering
    \includegraphics[width=4.5in]{background/accelerometers.png}
\end{figure}

\subsection{MARG Arrays and Inertial Measurement Units} \label{ssec:marg_imu}
Now, with the basics of each sensor in mind, we can combine them into a single package, called a Magnetic, Angular Rate, and Gravity (MARG) array.
Each of the above diagrams represent a single sensing axis, or Degree of Freedom (DOF).
In order for a MARG array to be useful in a 3D environment, it needs to have three sensing axes orthogonal to each other.
Each DOF will measure the X-, Y-, and Z-axis, respectively with the positive sensing direction according to the right hand rule.
By combining multiple tri-axial arrays, we can define different types of Inertial Measurement Units (IMUs), as shown in Table \ref{tab:imu_dofs}.
Typically, the more sensing axes, the more accurate the array will be (depending on the performance of the sensor fusion algorithm).
Note that a MARG array is an IMU with an integrated tri-axial magnetometer .

\begin{table}[h]
    \caption{Common definitions for IMUs of varying degrees of freedom.}
    \label{tab:imu_dofs}
    \centering
    \begin{tabular}{| c | c | c | c | c |}
        \hline
        DOFs & Accelerometer & Gyroscope & Magnetometer & Barometer \\
        \hline
        3-DOF & 3-axis\footnote[2]{can be either or} & 3-axis\footnote[2] & --- & --- \\
        6-DOF & 3-axis & 3-axis & --- & --- \\
        9-DOF & 3-axis & 3-axis & 3-axis & --- \\
        10-DOF & 3-axis & 3-axis & 3-axis & 1-axis \\
        \hline
    \end{tabular}
\end{table}

\begin{figure}
    \begin{fitbox}[frametitle=Aside: MEMS Technology]
        During the Apollo program, the ST124-M inertial measurement unit was developed that fed the Saturn V rocket's inertial characteristics to the main flight computer [CITE - ST124-M description].
        The IMU consisted of a tri-axial gyroscope array, redundant tri-axial accelerometer arrays, pendulums, and other sensors.
        While it was a technological marvel at the time, it was the size of a basketball and weighed about 45-65 kilograms.
        Modern cellphones with the same capability are orders of magnitude smaller, lighter, and cheaper; so, what happened?

        \begin{center}
            \includegraphics[height=3in]{images/background/Apollo_IMU_exhibit.jpg}
            % \caption[Test]{ST124-M inertial measurement unit courtesy of NASA \url{https://en.wikipedia.org/wiki/ST-124-M3_inertial_platform#/media/File:ST124-M_drawing.jpg}}
            \caption[ST124-M Outline]{ST124-M inertial measurement unit courtesy of NASA via Wikimedia Commons [CITE - Wikimedia]}
        \end{center}

        Modern manufacturing methods have enabled a new technology called Microelectromechanical Systems, or MEMS.
        MEMS are microscopic devices that perform an electromechanical function such as sensing acceleration, rotation rate, or magnetic fields [CITE - What is MEMS].
        They are the most common technology in modern sensor development and revolutionized the technology space by shrinking components down to the micro- and nanometer scales.
        Due to the advent of MEMS technology, devices such as accelerometers, magnetometers, gyroscopes, barometers, hygrometers, etc. have been shrunk down from large mechanical masterpieces to mass-producible products that fit within a few square millimeters of epoxy.
        This dramatically cheapened these devices and has allowed more products to integrate smart sensors into their designs.
    \end{fitbox}
\end{figure}

According to VectorNav [CITE], a producer of industrial-grade IMUS, the cheapest, least precise, and least accurate IMUs are considered "consumer-grade".
Every day smartphones, cheap commercial breakout boards, and even shipping crates have these devices on-board.
Consumer-grade IMUs can be bought for cents, dollars, or tens of dollars per unit in bulk and are ideal for mass production spaces where quantity is king over quality.
A step up from these sensors are "industrial-grade" IMUs. 
These are tens to hundreds of dollars per unit but are an order of magnitude more accurate and precise than their consumer counter parts.
This makes them desirable for the automotive and industrial sectors as they can assist in automation, control, and monitoring of expensive unmanned systems.
The "tactical-grade" sensors are even more robust and accurate than the previous tiers and have an appropriate military-industrial complex price tag to match. 
These devices will typically be used in applications where war fighters need precise guidance for munitions, or need to navigate their way through hazardous and GNSS-denied terrain.
Finally, the last major tier is the "navigation-grade" sensors. 
These sensors are extremely precise, extremely accurate, and cost more than some middle-class families will make in a year.
Primarily, these will be used in survey missions or on underwater vehicles where absolute precision and knowledge of their location is necessary.

\subsection{Global Positioning System} \label{ssec:gps}
The Global Position System, or GPS, is a constellation of high-altitude satellites that service most of the globe [CITE].
First developed by the US military for large scale maneuvers on the battlefield, GPS is a ubiquitous technology that is available in almost every device from smartphones to cars.
Each GPS satellite in orbit transmits the current time measured by their internal atomic clocks.
A GPS receiver on the ground can synchronize its own internal clock to the GPS time and wait for a satellite's transmission.
When the satellite time is received, the device can determine the difference between its clock and the satellite's report called the Time of Flight.
Since the Time of Flight can be assumed to be the constant speed of light, the GPS receiver can determine its distance to a satellite in a known orbit.
Repeat for at least three satellites, and a GPS receiver can triangulate its position to a reasonable circle of error of about 3-meters.

\begin{figure}[h!]
    \caption[GPS diagram]{Basic representation of how GPS communicates with a device to calculate its position on Earth.
    GPS satellites in orbit broadcast a time and using trigonometry and algebra, a device can calculate the distance to various satellites and triangulate its position.}
    \label{fig:gps}
    \centering
    \includegraphics[width=4.5in]{background/gps.png}
\end{figure}

\section{Sensor Fusion} \label{sec:sensor_fusion}
While having each individual sensor will give some data, they offer an incomplete picture of a body's orientation and movement through space.
An accelerometer can detect acceleration and determine pitch and roll using basic vector math, but it cannot determine yaw or heading.
Data from a gyroscope can be integrated over time to determine the sensor's attitude, but this method will drift over time and accumulate errors, it also cannot detect movement.
Magnetometers in a weak magnetic field, like Earth's, are not accurate enough to determine roll and pitch, but can easily provide heading.
Finally, GPS readings will provide position and velocity vectors, but typically update slowly and have a large circle of error and cannot determine attitude.
In order to make these sensors more effective for an inertial sensing application, we need to fuse the data feeds into a unified output that emphasizes the strength of each sensor, while mitigating their limitations.

\subsection{Attitude and Heading Reference System} \label{ssec:ahrs}
An Attitude and Heading Reference System (AHRS) is an IMU equipped with an accelerometer, gyroscope, and/or a magnetometer in all three axes.
The data streams from the sensors can be fused together with external information like GPS data and mathematical models to estimate a body's inertial orientation in three dimensional space.
The block diagram for this operation is shown in Figure \ref{fig:ahrs_design}.
Many algorithms exist to do this sensor fusion such as the Kalman filter [CITE - Kalman], the Mahony filter [CITE], the Madgwick filter [CITE], and the Fast Complimentary Filter [CITE].

\begin{figure}[h!]
    \caption[AHRS block diagram]{Basic block diagram of an AHRS. 
    The acceleration, rotation rates, and magnetic field readings are fused together in the IMU. 
    The AHRS can then apply a bias from the GPS course and corrections from a mathematical model of the system.}
    \label{fig:ahrs_design}
    \centering
    \includegraphics[height=1.3in]{background/ahrs.png}
\end{figure}

\subsection{Fast Complimentary Filter} \label{ssec:complimentary_filter}

\subsection{Kalman Filter} \label{ssec:kalman_filter}
The Kalman Filter is a recursive algorithm introduced in the 1960's as a method to track, estimate, and predict the state of a system and corresponding uncertainties [CITE].
This filter integrates a dynamic (linear) model of the system, control inputs, measurements, and biases/uncertainties into a single algorithm.
This effectively fuses together system inputs and responses and extrapolates what the system is currently doing and expected to do.
One key advantage of this algorithm is that it only requires the guess of the previous state to estimate the current state. 
This massively decreases the memory and processing costs as the history of inputs, measurements, and uncertainties does not need to be remembered or analyzed.
However, it does have some limitations when the sensor data is noisy or the control inputs cannot be linearly mapped to the system state.
Random errors in the sensor data may cause the filter to behave unpredictably and non-linearity prevents proper fusion entirely.

The algorithm works by taking uncertainties from sensor measurements and using those to determine the Kalman Gain factor, $K_n$.
This gain is the magnitude of the uncertainties from the previous estimate and measurement sources.

\begin{equation} \label{eq:kalman_gain}
    \begin{aligned}
        K_n &= \frac{\text{Estimate Uncertainty}}{\text{Estimate Uncertainty + Measurement Uncertainty}} \\
            &= \frac{p_{n,n-1}}{p_{n,n-1} + r_n}
    \end{aligned}
\end{equation}

The gain is then applied to a state update equation where the current estimate of the sensor values is determined.

\begin{equation} \label{eq:kalman_state_update_eq}
    \begin{aligned}
        \hat{x}_{n,n} &= \hat{x}_{n,n-1} + K_n(z_n - \hat{x}_{n,n-1}) \\
                      &= (1-K_n)\hat{x}_{n,n-1} + K_n z_n \\
    \end{aligned}
\end{equation}

On the first run of the algorithm, an initial state guess and uncertainty are introduced as a starting point for the state prediction equation.
This equation is the mathematical model of the system and, for example, can be the Newtonian equations of motion for a moving body.
This creates a prediction for the next state.
The algorithm can then output the current state, predicted next state, and associated uncertainties with reasonably high accuracy - assuming the filter is tuned.
Tuning the filter can be done by better quantifying the measurement errors and by tuning hyperparameter called "process noise variables".
These variables control how much the algorithm depends on the measurement or the previous estimate to make the current and future estimates.

\begin{figure}[h!]
    \caption[Kalman filter block diagram]{Basic block diagram of a Kalman filter.}
    \label{fig:kalman_filter}
    \centering
    \includegraphics[width=6in]{background/KalmanFilter.png}
\end{figure}

\subsection{Mahony Filter} \label{ssec:mahony_filter}
A popular AHRS sensor fusion algorithm is the Mahony filter [CITE].
This algorithm estimates the rotation rate, $\pmb{\omega}'$ of the body given the instantaneous rotation rate, $\pmb{\omega}$ and acceleration vector, \pmb{a}.
The error, $e$, is calculated as the cross-product between the current acceleration vector and the expected gravitational acceleration vector from the previous estimate.
The filter uses a Proportional-Integral (PI) controller to determine the filter error from the previous estimate and apply a bias, $\delta\omega$ to the current estimate.
By integrating $\omega'$, the estimated attitude can be determined.
The equation for a basic Mahony figure is given below:

\begin{gather}
    \label{eq:mahony_filter}
    \pmb{\omega}' = \pmb{\omega} + \left(K_p + K_i\frac{1}{s}\right) \pmb{a} \times \pmb{d} \\
    \begin{aligned}
        \text{where } & \pmb{\omega} \text{ is the instantaneous rotation rate vector} \\
        & K_p \text{ is the proportional controller constant} \\
        & K_i \text{ is the integral controller constant} \\
        & \pmb{a} \text{ is the instantaneous acceleration vector} \\
        & \pmb{d} \text{ is the estimated gravity vector from the previous estimate} \\
    \end{aligned} \notag
\end{gather}

This filter is used to fuse 6DOF IMU data, but is limited as the yaw orientation cannot be known or calculated; it also does not account for gyroscopic drift which decreases its accuracy over time.
Additionally, this filter primarily works in the Eulerian coordinate system, which causes singularities and discontinuities due to gimbal lock.

\subsection{Madgwick Filter} \label{ssec:madgewick_filter}
To address the issues of the Mahony filter, Madgewick [CITE] proposed a different orientation filter, based on Mahony's work, that used quaternions throughout the calculations and could integrate magnetometer data for yaw estimation.
The use of quaternions throughout the entire filter eliminates the Eulerian singularities and improves performance.
Additionally, Madgewick's implementation considers the estimation bias to be an optimization problem solved by gradient-descent.
The gradient-descent approach allows the algorithm to have a much higher performance at lower sample rates and incorporate online magnetic distortion and gyroscopic drift compensation.

The filter uses accelerometer and magnetometer measurements to estimate the body's tilt and yaw relative to the geodetic frame of reference.
This gives an estimate of the direction vector for the error vector.
The magnitude of the error is determined via gradient-descent and subtracted from the gyroscope readings along the direction vector.
This computes an accurate rate of change vector, $\pmb{\dot{q}_t}$, that can be numerically integrated to yield the estimated orientation vector, $\pmb{q_t}$.